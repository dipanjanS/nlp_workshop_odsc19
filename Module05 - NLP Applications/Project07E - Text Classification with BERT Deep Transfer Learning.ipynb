{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://colab.research.google.com/github/dipanjanS/nlp_workshop_odsc19/blob/master/Module05%20-%20NLP%20Applications/Project07E%20-%20Text%20Classification%20with%20BERT%20Deep%20Transfer%20Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_a-R2xFgtyUU"
   },
   "source": [
    "# Text Classification with BERT - Deep Transfer Learning\n",
    "\n",
    "![](https://i.imgur.com/MFd6n82.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-LbeQ0ZsowuZ"
   },
   "source": [
    "# Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "44WbTML-Z6JQ",
    "outputId": "6b2b8844-1dd7-497b-a5a2-664124f8b669"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: contractions in /usr/local/lib/python3.6/dist-packages (0.0.21)\n",
      "Requirement already satisfied: bert-tensorflow in /usr/local/lib/python3.6/dist-packages (1.0.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from bert-tensorflow) (1.12.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.28.1)\n",
      "Requirement already satisfied: tensorflow_hub in /usr/local/lib/python3.6/dist-packages (0.5.0)\n",
      "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_hub) (3.7.1)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_hub) (1.12.0)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_hub) (1.16.4)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.4.0->tensorflow_hub) (41.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install contractions\n",
    "!pip install bert-tensorflow\n",
    "!pip install tqdm\n",
    "!pip install tensorflow_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ICsFrmtQo0ij"
   },
   "source": [
    "# Load Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cXGxumq_Z6JT"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as tf_hub\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "from bert.tokenization import FullTokenizer\n",
    "import tqdm\n",
    "from tensorflow.keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.set_random_seed(SEED)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sMzS6N-xo3HW"
   },
   "source": [
    "# GPU Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "aCnPGZk5Z6JV",
    "outputId": "0abbd366-3133-4b98-cff5-95c29dc5f758"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0\n",
      "0.5.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "print(tf_hub.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "oZGV4XE3Z6JY",
    "outputId": "9f7deb9a-1a30-4380-f4ad-51c4c0b47f82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "print(tf.test.is_gpu_available())\n",
    "print(tf.test.gpu_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "xWeT_CIZZ6Jb",
    "outputId": "367a62cd-4ec5-4216-8b5e-6b9db5a876b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Aug  6 05:47:55 2019       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 418.67       Driver Version: 410.79       CUDA Version: 10.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   67C    P0    31W /  70W |    129MiB / 15079MiB |      1%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4X7iDM4Vo5g7"
   },
   "source": [
    "# Load and View Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "nyMDImuTZ6Je",
    "outputId": "6ca0243f-4d88-41a4-a4e0-b2ba0ff8466b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 2 columns):\n",
      "review       50000 non-null object\n",
      "sentiment    50000 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 781.3+ KB\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv('https://github.com/dipanjanS/nlp_workshop_dhs18/raw/master/Unit%2011%20-%20Sentiment%20Analysis%20-%20Unsupervised%20Learning/movie_reviews.csv.bz2', compression='bz2')\n",
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "iu_aqmqiZ6Jh",
    "outputId": "07cd2853-595d-4986-c5f7-e275c9f5d203"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  One of the other reviewers has mentioned that ...          1\n",
       "1  A wonderful little production. <br /><br />The...          1\n",
       "2  I thought this was a wonderful way to spend ti...          1\n",
       "3  Basically there's a family where a little boy ...          0\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...          1"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['sentiment'] = [1 if sentiment == 'positive' else 0 \n",
    "                            for sentiment in dataset['sentiment'].values]\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "msZ9sC9eo8RE"
   },
   "source": [
    "# Prepare Train, Validation and Test Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "AcoddPTUZ6Jj",
    "outputId": "9c566f58-cd9e-4105-e37f-897463be49d2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5000, 2), (5000, 2), (15000, 2))"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = dataset.iloc[:5000]\n",
    "val_df = dataset.iloc[30000:35000]\n",
    "test_df = dataset.iloc[35000:]\n",
    "\n",
    "train_df.shape, val_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tfTo2g1vZ6Jm"
   },
   "outputs": [],
   "source": [
    "train_text = train_df['review'].tolist()\n",
    "train_labels = train_df['sentiment'].tolist()\n",
    "\n",
    "val_text = val_df['review'].tolist()\n",
    "val_labels = val_df['sentiment'].tolist()\n",
    "\n",
    "test_text = test_df['review'].tolist()\n",
    "test_labels = test_df['sentiment'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ClxtbLbYpAiA"
   },
   "source": [
    "# Minimal Text Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "tj4d4rSQZ6Jr",
    "outputId": "5ede03eb-39dc-4e4e-dfec-25342522d1df"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:01<00:00, 3656.50it/s]\n",
      "100%|██████████| 5000/5000 [00:01<00:00, 3722.48it/s]\n",
      "100%|██████████| 15000/15000 [00:04<00:00, 3695.23it/s]\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import tqdm\n",
    "import re\n",
    "\n",
    "def strip_html_tags(text):\n",
    "  soup = BeautifulSoup(text, \"html.parser\")\n",
    "  [s.extract() for s in soup(['iframe', 'script'])]\n",
    "  stripped_text = soup.get_text()\n",
    "  stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text)\n",
    "  return stripped_text\n",
    "\n",
    "def clean_docs(docs):\n",
    "  norm_docs = []\n",
    "  for doc in tqdm.tqdm(docs):\n",
    "    doc = strip_html_tags(doc)\n",
    "    norm_docs.append(doc)\n",
    "  return norm_docs\n",
    "\n",
    "train_text = clean_docs(train_text)\n",
    "val_text = clean_docs(val_text)\n",
    "test_text = clean_docs(test_text) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VUyYV-iIpD21"
   },
   "source": [
    "# BERT Data Preparation\n",
    "\n",
    "We'll need to transform our data into a format BERT understands. This involves two steps. First, we create InputExample's based on the constructor provided in the BERT library (we model based on that).\n",
    "\n",
    "- `text_a` is the text we want to classify, which in this case, is the review field in our Dataframe.\n",
    "- `text_b` is used if we're training a model to understand the relationship between sentences (i.e. is `text_b` a translation of `text_a`? Is `text_b` an answer to the question asked by `text_a`?). This doesn't apply to our task, so we can leave `text_b` blank.\n",
    "- `label` is the label for our example, i.e. 1, 0\n",
    "\n",
    "\n",
    "Next, we need to preprocess our data so that it matches the data BERT was trained on. For this, we'll need to do a couple of things.\n",
    "\n",
    "- Lowercase our text (if we're using a BERT lowercase model)\n",
    "- Tokenize it (i.e. \"sally says hi\" -> [\"sally\", \"says\", \"hi\"])\n",
    "- Break words into WordPieces (i.e. \"calling\" -> [\"call\", \"##ing\"])\n",
    "- Map our words to indexes using a vocab file that BERT provides\n",
    "- Add special \"CLS\" and \"SEP\" tokens (see the readme)\n",
    "- Append \"index\" and \"segment\" tokens to each input (see the BERT paper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KCmYB8-OrKDR"
   },
   "source": [
    "## BERT InputExamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MZ-eiTcDZ6Jw"
   },
   "outputs": [],
   "source": [
    "class PaddingInputExample(object):\n",
    "  pass\n",
    "    \n",
    "    \n",
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
    "\n",
    "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
    "        \n",
    "        \"\"\"Constructs a InputExample.\n",
    "    Args:\n",
    "      guid: Unique id for the example.\n",
    "      text_a: string. The untokenized text of the first sequence. For single\n",
    "        sequence tasks, only this sequence must be specified.\n",
    "      text_b: (Optional) string. The untokenized text of the second sequence.\n",
    "        Only must be specified for sequence pair tasks.\n",
    "      label: (Optional) string. The label of the example. This should be\n",
    "        specified for train and dev examples, but not for test examples.\n",
    "    \"\"\"\n",
    "        self.guid = guid\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "        self.label = label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5YY5IgKlq0Kq"
   },
   "source": [
    "# BERT Tokenization\n",
    "\n",
    "The BERT model we're using expects lowercase data (that's what stored in the tokenization_info parameter do_lower_case. Besides this, we also loaded BERT's vocab file. Finally, we created a tokenizer, which breaks words into word pieces.\n",
    "\n",
    "Word Piece Tokenizer is based on [Byte Pair Encodings (BPE)](https://www.aclweb.org/anthology/P16-1162).\n",
    "\n",
    "WordPiece and BPE are two similar and commonly used techniques to segment words into subword-level in NLP tasks. In both cases, the vocabulary is initialized with all the individual characters in the language, and then the most frequent/likely combinations of the symbols in the vocabulary are iteratively added to the vocabulary.\n",
    "\n",
    "magine that the model sees the word walking. Unless this word occurs at least a few times in the training corpus, the model can't learn to deal with this word very well. However, it may have the words walked, walker, walks, each occurring only a few times. Without subword segmentation, all these words are treated as completely different words by the model.\n",
    "\n",
    "However, if these get segmented as walk@@ ing, walk@@ ed, etc., notice that all of them will now have walk@@ in common, which will occur much frequently while training, and the model might be able to learn more about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fe--ekRZZ6Jy"
   },
   "outputs": [],
   "source": [
    "def create_tokenizer_from_hub_module(bert_path):\n",
    "    \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
    "    bert_module =  tf_hub.Module(bert_path)\n",
    "    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
    "    vocab_file, do_lower_case = sess.run(\n",
    "        [\n",
    "            tokenization_info[\"vocab_file\"],\n",
    "            tokenization_info[\"do_lower_case\"],\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return FullTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "heH4-emIshLI"
   },
   "source": [
    "# BERT Input Feature Extractor\n",
    "\n",
    "Follows the InputExample instance format of converting each text into:\n",
    "- guid\n",
    "- text_a\n",
    "- text_b\n",
    "- label (optional)\n",
    "\n",
    "![](https://i.imgur.com/sY0xQih.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wZA98b55Z6J0"
   },
   "outputs": [],
   "source": [
    "def convert_text_to_examples(texts, labels):\n",
    "    \"\"\"Create InputExamples\"\"\"\n",
    "    InputExamples = []\n",
    "    for text, label in zip(texts, labels):\n",
    "        InputExamples.append(\n",
    "            InputExample(guid=None, text_a=text, text_b=None, label=label)\n",
    "        )\n",
    "    return InputExamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0bQkkTkzZ6J3"
   },
   "outputs": [],
   "source": [
    "def convert_single_example(tokenizer, example, max_seq_length=256):\n",
    "    \"\"\"Converts a single `InputExample` into a single `InputFeatures`.\"\"\"\n",
    "\n",
    "    if isinstance(example, PaddingInputExample):\n",
    "        input_ids = [0] * max_seq_length\n",
    "        input_mask = [0] * max_seq_length\n",
    "        segment_ids = [0] * max_seq_length\n",
    "        label = 0\n",
    "        return input_ids, input_mask, segment_ids, label\n",
    "\n",
    "    tokens_a = tokenizer.tokenize(example.text_a)\n",
    "    if len(tokens_a) > max_seq_length - 2:\n",
    "        tokens_a = tokens_a[0 : (max_seq_length - 2)]\n",
    "\n",
    "    tokens = []\n",
    "    segment_ids = []\n",
    "    tokens.append(\"[CLS]\")\n",
    "    segment_ids.append(0)\n",
    "    for token in tokens_a:\n",
    "        tokens.append(token)\n",
    "        segment_ids.append(0)\n",
    "    tokens.append(\"[SEP]\")\n",
    "    segment_ids.append(0)\n",
    "\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "    # tokens are attended to.\n",
    "    input_mask = [1] * len(input_ids)\n",
    "\n",
    "    # Zero-pad up to the sequence length.\n",
    "    while len(input_ids) < max_seq_length:\n",
    "        input_ids.append(0)\n",
    "        input_mask.append(0)\n",
    "        segment_ids.append(0)\n",
    "\n",
    "    assert len(input_ids) == max_seq_length\n",
    "    assert len(input_mask) == max_seq_length\n",
    "    assert len(segment_ids) == max_seq_length\n",
    "\n",
    "    return input_ids, input_mask, segment_ids, example.label\n",
    "\n",
    "def convert_examples_to_features(tokenizer, examples, max_seq_length=256):\n",
    "    \"\"\"Convert a set of `InputExample`s to a list of `InputFeatures`.\"\"\"\n",
    "\n",
    "    input_ids, input_masks, segment_ids, labels = [], [], [], []\n",
    "    for example in tqdm.tqdm(examples, desc=\"Converting examples to features\"):\n",
    "        input_id, input_mask, segment_id, label = convert_single_example(\n",
    "            tokenizer, example, max_seq_length\n",
    "        )\n",
    "        input_ids.append(input_id)\n",
    "        input_masks.append(input_mask)\n",
    "        segment_ids.append(segment_id)\n",
    "        labels.append(label)\n",
    "    return (\n",
    "        np.array(input_ids),\n",
    "        np.array(input_masks),\n",
    "        np.array(segment_ids),\n",
    "        np.array(labels).reshape(-1, 1),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8lRGgUhAuYyp"
   },
   "source": [
    "# Loading BERT Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4fJ4QsIIZ6J5"
   },
   "outputs": [],
   "source": [
    "# Initialize session\n",
    "sess = tf.Session()\n",
    "\n",
    "# Params for bert model and tokenization\n",
    "BERT_PATH = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
    "MAX_SEQ_LENGTH = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "fDycqOdjZ6J8",
    "outputId": "150278f6-68ed-465d-b399-2a9aa4a17d5d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "I0806 05:48:11.923502 140647828572032 saver.py:1499] Saver not created because there are no variables in the graph to restore\n",
      "W0806 05:48:12.745496 140647828572032 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Instantiate tokenizer\n",
    "tokenizer = create_tokenizer_from_hub_module(bert_path=BERT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J_VnLNBWuk8J"
   },
   "source": [
    "# Convert Text Data to BERT Input Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NMF8qQ7fZ6J-"
   },
   "outputs": [],
   "source": [
    "# Convert data to InputExample format\n",
    "train_examples = convert_text_to_examples(train_text, train_labels)\n",
    "val_examples = convert_text_to_examples(val_text, val_labels)\n",
    "test_examples = convert_text_to_examples(test_text, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "8h1sNHWUZ6KB",
    "outputId": "bbf1f884-2d73-4262-a0a8-44f3a1f8b965"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting examples to features: 100%|██████████| 5000/5000 [00:25<00:00, 196.50it/s]\n",
      "Converting examples to features: 100%|██████████| 5000/5000 [00:24<00:00, 200.72it/s]\n",
      "Converting examples to features: 100%|██████████| 15000/15000 [01:15<00:00, 199.39it/s]\n"
     ]
    }
   ],
   "source": [
    "(train_input_ids, train_input_masks, \n",
    " train_segment_ids, train_labels) =  convert_examples_to_features(tokenizer=tokenizer, \n",
    "                                                                  examples=train_examples, \n",
    "                                                                  max_seq_length=MAX_SEQ_LENGTH)\n",
    "\n",
    "(val_input_ids, val_input_masks, \n",
    " val_segment_ids, val_labels) =  convert_examples_to_features(tokenizer=tokenizer, \n",
    "                                                              examples=val_examples, \n",
    "                                                              max_seq_length=MAX_SEQ_LENGTH)\n",
    "\n",
    "(test_input_ids, test_input_masks, \n",
    " test_segment_ids, test_labels) =  convert_examples_to_features(tokenizer=tokenizer, \n",
    "                                                                examples=test_examples, \n",
    "                                                                max_seq_length=MAX_SEQ_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "uxUvqAcWZ6KH",
    "outputId": "50ea38cc-5c00-4b22-ea03-afa66990cb5f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5000, 512), (5000, 512), (15000, 512))"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input_ids.shape, val_input_ids.shape, test_input_ids.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VfuufEMKupyX"
   },
   "source": [
    "# Load BERT Model\n",
    "\n",
    "![](https://i.imgur.com/84Din4P.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "YZZkKQZsZ6KP",
    "outputId": "98182079-bbe4-4b8a-d5ef-84dda7a22e78"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow_hub.module.Module at 0x7fea6cc89b00>"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm = tf_hub.Module(BERT_PATH, trainable=True, name=f\"bert_module\")\n",
    "bm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Vd98ZHIOZ6KV",
    "outputId": "52d94408-e168-4bdb-cbe6-10f45b2d1e26"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "204"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bm.variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F89mn-thvJ-_"
   },
   "source": [
    "# Custom BERT Layer to fine-tune BERT Encoder Layers\n",
    "\n",
    "![](https://i.imgur.com/gnIxACX.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9YptN2RqZ6Ka"
   },
   "outputs": [],
   "source": [
    "class BertLayer(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, bert_path, n_fine_tune_encoders=10, **kwargs,):\n",
    "        \n",
    "        self.n_fine_tune_encoders = n_fine_tune_encoders\n",
    "        self.trainable = True\n",
    "        self.output_size = 768\n",
    "        self.bert_path = bert_path\n",
    "        super(BertLayer, self).__init__(**kwargs)\n",
    "\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.bert = tf_hub.Module(self.bert_path,\n",
    "                                  trainable=self.trainable, \n",
    "                                  name=f\"{self.name}_module\")\n",
    "\n",
    "        # Remove unused layers\n",
    "        trainable_vars = self.bert.variables\n",
    "        trainable_vars = [var for var in trainable_vars \n",
    "                                  if not \"/cls/\" in var.name]\n",
    "        trainable_layers = [\"embeddings\", \"pooler/dense\"]\n",
    "\n",
    "\n",
    "        # Select how many layers to fine tune\n",
    "        for i in range(self.n_fine_tune_encoders+1):\n",
    "            trainable_layers.append(f\"encoder/layer_{str(10 - i)}\")\n",
    "\n",
    "        # Update trainable vars to contain only the specified layers\n",
    "        trainable_vars = [var for var in trainable_vars\n",
    "                                  if any([l in var.name \n",
    "                                              for l in trainable_layers])]\n",
    "\n",
    "        # Add to trainable weights\n",
    "        for var in trainable_vars:\n",
    "            self._trainable_weights.append(var)\n",
    "\n",
    "        for var in self.bert.variables:\n",
    "            if var not in self._trainable_weights:# and 'encoder/layer' not in var.name:\n",
    "                self._non_trainable_weights.append(var)\n",
    "        print('Trainable layers:', len(self._trainable_weights))\n",
    "        print('Non Trainable layers:', len(self._non_trainable_weights))\n",
    "\n",
    "        super(BertLayer, self).build(input_shape)\n",
    "\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        \n",
    "        inputs = [K.cast(x, dtype=\"int32\") for x in inputs]\n",
    "        input_ids, input_mask, segment_ids = inputs\n",
    "        bert_inputs = dict(input_ids=input_ids, \n",
    "                           input_mask=input_mask, \n",
    "                           segment_ids=segment_ids)\n",
    "        \n",
    "        pooled = self.bert(inputs=bert_inputs, \n",
    "                           signature=\"tokens\", \n",
    "                           as_dict=True)[\"pooled_output\"]\n",
    "\n",
    "        return pooled\n",
    "\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_size) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jPWKg6zbvctA"
   },
   "source": [
    "# Integrate BERT Model for Downstream Classification Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "en6zMDsNZ6Kd"
   },
   "outputs": [],
   "source": [
    "# Build model\n",
    "def build_model(bert_path, max_seq_length, n_fine_tune_encoders=10): \n",
    "    \n",
    "    inp_id = tf.keras.layers.Input(shape=(max_seq_length,), name=\"input_ids\")\n",
    "    inp_mask = tf.keras.layers.Input(shape=(max_seq_length,), name=\"input_masks\")\n",
    "    inp_segment = tf.keras.layers.Input(shape=(max_seq_length,), name=\"segment_ids\")\n",
    "    bert_inputs = [inp_id, inp_mask, inp_segment]\n",
    "    \n",
    "    bert_output = BertLayer(bert_path=bert_path, \n",
    "                            n_fine_tune_encoders=n_fine_tune_encoders)(bert_inputs)\n",
    "    \n",
    "    dense = tf.keras.layers.Dense(256, activation='relu')(bert_output)\n",
    "    pred = tf.keras.layers.Dense(1, activation='sigmoid')(dense)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=bert_inputs, outputs=pred)\n",
    "    model.compile(loss='binary_crossentropy', \n",
    "                  optimizer=tf.keras.optimizers.Adam(lr=2e-5), \n",
    "                  metrics=['accuracy'])    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4TXBWH22viaf"
   },
   "source": [
    "# Build BERT Classifier Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eMz7b8evZ6Kf"
   },
   "outputs": [],
   "source": [
    "def initialize_vars(sess):\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.tables_initializer())\n",
    "    K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "yR7F0QjIZ6Kh",
    "outputId": "606424c6-ba79-4373-f7f1-a72d3af759be"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_SEQ_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190
    },
    "colab_type": "code",
    "id": "z1hHURHlZ6Kj",
    "outputId": "b4c5faec-eede-4ac1-db53-8378389151d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable layers: 199\n",
      "Non Trainable layers: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0806 05:50:59.351067 140647828572032 saver.py:1499] Saver not created because there are no variables in the graph to restore\n",
      "W0806 05:51:05.398488 140647828572032 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0806 05:51:05.472497 140647828572032 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "model = build_model(bert_path=BERT_PATH, max_seq_length=MAX_SEQ_LENGTH, n_fine_tune_encoders=10)\n",
    "\n",
    "# Instantiate variables\n",
    "initialize_vars(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "id": "4bwjo1wGZ6Kl",
    "outputId": "fda7a1c4-a152-4ea0-b079-974d97703f12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_masks (InputLayer)        [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert_layer (BertLayer)          (None, 768)          110104890   input_ids[0][0]                  \n",
      "                                                                 input_masks[0][0]                \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          196864      bert_layer[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            257         dense[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 110,302,011\n",
      "Trainable params: 109,679,361\n",
      "Non-trainable params: 622,650\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tbU4i99HvmFM"
   },
   "source": [
    "# Train BERT Classifier (fine-tune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "LRDljOeHZ6Kq",
    "outputId": "f2b18f7c-b796-42ce-a7a7-a8af5d044c09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5000 samples, validate on 5000 samples\n",
      "Epoch 1/2\n",
      "5000/5000 [==============================] - 819s 164ms/sample - loss: 0.3019 - acc: 0.8696 - val_loss: 0.2122 - val_acc: 0.9136\n",
      "Epoch 2/2\n",
      "5000/5000 [==============================] - 812s 162ms/sample - loss: 0.1035 - acc: 0.9650 - val_loss: 0.2317 - val_acc: 0.9188\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fea5dadf198>"
      ]
     },
     "execution_count": 29,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    [train_input_ids, train_input_masks, train_segment_ids], \n",
    "    train_labels,\n",
    "    validation_data=([val_input_ids, val_input_masks, val_segment_ids], val_labels),\n",
    "    epochs=2,\n",
    "    batch_size=15,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c9IDmLROvpzj"
   },
   "source": [
    "# Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "AXHqW3bIZ6Kt",
    "outputId": "53aba0c5-41c5-4add-cb1a-7f1cf30d403f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000/15000 [==============================] - 661s 44ms/sample\n"
     ]
    }
   ],
   "source": [
    "test_predictions = model.predict(x=[test_input_ids, \n",
    "                                    test_input_masks, \n",
    "                                    test_segment_ids],\n",
    "                                 batch_size=100,\n",
    "                                 verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TS-tSX9_Z6Ky"
   },
   "outputs": [],
   "source": [
    "test_predictions = test_predictions.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C5yNarXpZ6K1"
   },
   "outputs": [],
   "source": [
    "test_pred_labels = [1 if prob > 0.5 else 0 for prob in test_predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "5qoCPNMVZ6K3",
    "outputId": "cb89dd16-7862-46e7-ef4e-e408d27eebcf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.92      0.92      7490\n",
      "           1       0.92      0.92      0.92      7510\n",
      "\n",
      "    accuracy                           0.92     15000\n",
      "   macro avg       0.92      0.92      0.92     15000\n",
      "weighted avg       0.92      0.92      0.92     15000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_true=test_labels, y_pred=test_pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "colab_type": "code",
    "id": "L4dZNemXZ6K6",
    "outputId": "e9ae8db8-9713-420c-945a-557a9a3ffae5"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXUAAAEKCAYAAADticXcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nbmaWI/8PJvmu8tOT5SAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "with tf.Session() as session:\n",
    "    cm = tf.confusion_matrix(test_labels, test_pred_labels).eval()\n",
    "\n",
    "LABELS = ['negative', 'positive']\n",
    "sns.heatmap(cm, annot=True, xticklabels=LABELS, yticklabels=LABELS, fmt='g')\n",
    "xl = plt.xlabel(\"Predicted\")\n",
    "yl = plt.ylabel(\"Actuals\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8koPEuM4Z6Lv"
   },
   "source": [
    "# Take Home - Train and fine-tune BERT on the full training data\n",
    "\n",
    "Here we did see BERT's power by training the model on only 5000 examples. But you can bump up the performance even more by using the full training data.\n",
    "\n",
    "For reference, use [this notebook](https://github.com/fabric8-analytics/openshift-probable-vulnerabilities/blob/master/notebooks/modeling/deep_learning_models/phase2_transformer_models/bert_sentiment_analysis_benchmark_models/BERT%20-%20TF%20Keras%20implementation%20-%20Sentiment%20Analysis-Preprocessed%20Text-Seq512.ipynb)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Text Classification with BERT - Deep Transfer Learning.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
